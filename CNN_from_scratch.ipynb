{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ConvConnectedLayer:\n",
    "    def __init__(self, filter_count, kernel_shape, stride = 1,\n",
    "                 #weights = False, biases = False, \n",
    "                 activation_type = 'relu'):\n",
    "        \n",
    "        self.kernel_shape = kernel_shape\n",
    "        self.stride = stride\n",
    "        self.weights = np.random.rand(kernel_shape[0], \n",
    "                                      kernel_shape[1]) #use rand to init, use 3rd dim for filter count?\n",
    "        \n",
    "        self.activation_type = activation_type\n",
    "        if self.activation_type == 'linear':\n",
    "            self.activation_function = lambda x : x\n",
    "        elif self.activation_type == 'relu':\n",
    "            self.activation_function = lambda x : np.maximum(x, 0)\n",
    "        elif self.activation_type == 'sigmoid':\n",
    "            self.activation_function = lambda x : 1 / (1 + np.exp(-1 * x))\n",
    "        elif self.activation_type == 'softmax':\n",
    "            self.activation_function = lambda x : np.exp(x) / np.sum(np.exp(x), axis = 1)\n",
    "        else:\n",
    "            raise Exception('Invalid activation function')\n",
    "        \n",
    "        self.biases = np.random.rand(1, filter_count)\n",
    "            \n",
    "    def convolve(self, region):\n",
    "        weighted_output = np.sum(np.multiply(region, self.weights)) + self.biases\n",
    "        return weighted_output[0][0]\n",
    "    \n",
    "    def forward_prop(self, input_data, padding = 'same', activation = True):\n",
    "        #padding for stride 1\n",
    "        orig_shape = input_data.shape\n",
    "        if (padding == 'same'):\n",
    "            #pad right edge\n",
    "            input_data = np.concatenate([input_data, np.zeros((input_data.shape[0], self.kernel_shape[1] - 1))], \n",
    "                                        axis = 1)\n",
    "            #pad bottom edge\n",
    "            input_data = np.concatenate([input_data, np.zeros((self.kernel_shape[0] - 1, input_data.shape[1]))], \n",
    "                                        axis = 0)\n",
    "        elif padding != 'none':\n",
    "            raise Exception('Padding not supported')\n",
    "            \n",
    "    \n",
    "        output = []\n",
    "        for i in range(0, orig_shape[0], self.stride):\n",
    "            row = []\n",
    "            for j in range(0, orig_shape[1], self.stride):\n",
    "                row.append(self.convolve(region = input_data[i : i + self.kernel_shape[0],\n",
    "                                                             j : j + self.kernel_shape[1]]\n",
    "                                        )\n",
    "                          )\n",
    "            output.append(row)\n",
    "            \n",
    "        if activation:\n",
    "            return self.activation_function(np.array(output))\n",
    "        else:\n",
    "            return np.array(output)\n",
    "    \n",
    "    def back_prop_output(self, input_data, cost_grad, learning_rate = 0.001, padding = 'same'):\n",
    "        if self.activation_type == 'linear':\n",
    "            activation_grad = 1\n",
    "        elif self.activation_type == 'relu':\n",
    "            relu_grad = map(lambda x : 1 if x > 0 else -0.1, \n",
    "                            self.forward_prop(input_layer, activation = False).flatten())\n",
    "            \n",
    "            activation_grad = np.array([list(relu_grad)])\n",
    "        return activation_grad\n",
    "        #padding for stride 1\n",
    "        orig_shape = input_data.shape\n",
    "        if (padding == 'same'):\n",
    "            #pad right edge\n",
    "            input_data = np.concatenate([input_data, np.zeros((input_data.shape[0], self.kernel_shape[1] - 1))], \n",
    "                                        axis = 1)\n",
    "            #pad bottom edge\n",
    "            input_data = np.concatenate([input_data, np.zeros((self.kernel_shape[0] - 1, input_data.shape[1]))], \n",
    "                                        axis = 0)\n",
    "            \n",
    "            #pad right edge\n",
    "            activation_grad = np.concatenate([activation_grad, \n",
    "                                              np.zeros((activation_grad.shape[0], self.kernel_shape[1] - 1))], \n",
    "                                        axis = 1)\n",
    "            #pad bottom edge\n",
    "            activation_grad = np.concatenate([activation_grad, \n",
    "                                              np.zeros((self.kernel_shape[0] - 1, activation_grad.shape[1]))], \n",
    "                                        axis = 0)\n",
    "        elif padding != 'none':\n",
    "            raise Exception('Padding not supported')\n",
    "            \n",
    "    \n",
    "        weights_update = np.zeros(self.weights.shape)\n",
    "        for i in range(0, orig_shape[0], self.stride):\n",
    "            for j in range(0, orig_shape[1], self.stride):\n",
    "                region = input_data[i : i + self.kernel_shape[0],\n",
    "                                    j : j + self.kernel_shape[1]]\n",
    "                activation_grad_region = activation_grad[i : i + self.kernel_shape[0],\n",
    "                                                         j : j + self.kernel_shape[1]]\n",
    "                weights_update += np.multiply(region, activation_grad_region)\n",
    "        weights_update /= 16\n",
    "        weights_update *= cost_grad\n",
    "\n",
    "        self.weights += -1 * learning_rate * weights_update\n",
    "        self.biases += -1 * learning_rate * cost_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1426177437461167"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer = np.array([[1, 1, 0, 0], \n",
    "                        [1, 1, 0, 0], \n",
    "                        [0, 0, 1, 1], \n",
    "                        [0, 0, 1, 1],\n",
    "                       ])\n",
    "\n",
    "layer_1 = ConvConnectedLayer(filter_count = 1, kernel_shape = [2, 2], stride = 1,\n",
    "                              activation_type = 'relu'\n",
    "                             )\n",
    "\n",
    "# layer_2 = ConvConnectedLayer(filter_count = 1, kernel_shape = [2, 2], stride = 1,\n",
    "#                               activation_type = 'relu'\n",
    "#                              )\n",
    "np.mean(layer_1.forward_prop(input_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_2.back_prop_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1426177437461167\n",
      "1.1426177437461167\n",
      "1.1426177437461167\n",
      "1.1426177437461167\n",
      "1.1426177437461167\n",
      "1.1426177437461167\n",
      "1.1426177437461167\n",
      "1.1426177437461167\n",
      "1.1426177437461167\n",
      "1.1426177437461167\n"
     ]
    }
   ],
   "source": [
    "target = np.array([[-1]])\n",
    "for i in range(10):\n",
    "    cost_grad = -2 * (target - np.mean(layer_1.forward_prop(input_layer)))\n",
    "    layer_1.back_prop_output(input_layer, cost_grad, learning_rate = 0.5)\n",
    "    print(np.mean(layer_1.forward_prop(input_layer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.30721707, 0.74735136],\n",
       "       [0.74735136, 2.30721707]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_1.forward_prop(input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9682848719270942"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(layer_1.forward_prop(input_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05267091, 0.34828919],\n",
       "       [0.04212482, 0.20214394]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_1.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66860029])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_1.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_1.forward_prop(input_layer, stride = 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.91687232],\n",
       "        [0.91687232],\n",
       "        [0.91687232],\n",
       "        [0.91687232]],\n",
       "\n",
       "       [[0.91687232],\n",
       "        [0.91687232],\n",
       "        [0.91687232],\n",
       "        [0.91687232]],\n",
       "\n",
       "       [[0.91687232],\n",
       "        [0.91687232],\n",
       "        [0.91687232],\n",
       "        [0.91687232]],\n",
       "\n",
       "       [[0.91687232],\n",
       "        [0.91687232],\n",
       "        [0.91687232],\n",
       "        [0.91687232]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_1.forward_prop(input_layer, stride = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.10338361"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.92360386 + 0.17977975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_layer = FullyConnectedLayer(3, 2,\n",
    "                              activation_type = 'linear'\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target:\n",
      "[[-100  100]]\n",
      "[[-99.99995287 100.00004762]]\n",
      "weights:\n",
      "[[-1.63932624  2.55927905]\n",
      " [-0.52741058  1.49169219]\n",
      " [-5.70839765  5.10540074]]\n",
      "[[1.93412548 0.93371711 5.22570308]\n",
      " [2.13425639 0.81719704 4.92769156]]\n",
      "preds:\n",
      "[[-99.99997995 100.00002026]]\n",
      "weights:\n",
      "[[-1.63932648  2.55927881]\n",
      " [-0.5274107   1.49169208]\n",
      " [-5.70839827  5.10540012]]\n",
      "[[1.93412544 0.93371707 5.2257031 ]\n",
      " [2.13425635 0.81719701 4.92769158]]\n",
      "preds:\n",
      "[[-99.99999147 100.00000862]]\n",
      "weights:\n",
      "[[-1.63932658  2.5592787 ]\n",
      " [-0.52741074  1.49169203]\n",
      " [-5.70839853  5.10539985]]\n",
      "[[1.93412543 0.93371705 5.22570311]\n",
      " [2.13425633 0.81719699 4.92769159]]\n",
      "preds:\n",
      "[[-99.99999637 100.00000367]]\n",
      "weights:\n",
      "[[-1.63932662  2.55927866]\n",
      " [-0.52741076  1.49169201]\n",
      " [-5.70839864  5.10539974]]\n",
      "[[1.93412542 0.93371705 5.22570311]\n",
      " [2.13425633 0.81719698 4.9276916 ]]\n",
      "preds:\n",
      "[[-99.99999846 100.00000156]]\n",
      "weights:\n",
      "[[-1.63932664  2.55927864]\n",
      " [-0.52741077  1.491692  ]\n",
      " [-5.70839869  5.10539969]]\n",
      "[[1.93412542 0.93371704 5.22570311]\n",
      " [2.13425632 0.81719698 4.9276916 ]]\n",
      "preds:\n",
      "[[-99.99999934 100.00000066]]\n",
      "weights:\n",
      "[[-1.63932665  2.55927863]\n",
      " [-0.52741078  1.49169199]\n",
      " [-5.70839871  5.10539967]]\n",
      "[[1.93412541 0.93371704 5.22570311]\n",
      " [2.13425632 0.81719698 4.9276916 ]]\n",
      "preds:\n",
      "[[-99.99999972 100.00000028]]\n",
      "weights:\n",
      "[[-1.63932665  2.55927863]\n",
      " [-0.52741078  1.49169199]\n",
      " [-5.70839872  5.10539966]]\n",
      "[[1.93412541 0.93371704 5.22570311]\n",
      " [2.13425632 0.81719698 4.9276916 ]]\n",
      "preds:\n",
      "[[-99.99999988 100.00000012]]\n",
      "weights:\n",
      "[[-1.63932665  2.55927863]\n",
      " [-0.52741078  1.49169199]\n",
      " [-5.70839872  5.10539966]]\n",
      "[[1.93412541 0.93371704 5.22570311]\n",
      " [2.13425632 0.81719698 4.9276916 ]]\n",
      "preds:\n",
      "[[-99.99999995 100.00000005]]\n",
      "weights:\n",
      "[[-1.63932665  2.55927863]\n",
      " [-0.52741078  1.49169199]\n",
      " [-5.70839873  5.10539965]]\n",
      "[[1.93412541 0.93371704 5.22570311]\n",
      " [2.13425632 0.81719698 4.9276916 ]]\n",
      "preds:\n",
      "[[-99.99999998 100.00000002]]\n",
      "weights:\n",
      "[[-1.63932665  2.55927863]\n",
      " [-0.52741078  1.49169199]\n",
      " [-5.70839873  5.10539965]]\n",
      "[[1.93412541 0.93371704 5.22570311]\n",
      " [2.13425632 0.81719698 4.9276916 ]]\n",
      "preds:\n",
      "[[-99.99999999 100.00000001]]\n"
     ]
    }
   ],
   "source": [
    "target = np.array([[-100, 100]])\n",
    "print('target:')\n",
    "print(target)\n",
    "print(output_layer.forward_prop(layer_1.forward_prop(input_layer)))\n",
    "for i in range(10):\n",
    "    output_gradient = output_layer.back_prop_output(layer_1.forward_prop(input_layer), target, 'square_loss', \n",
    "                                                    learning_rate = 0.001, train = True)\n",
    "    layer_1.back_prop_hidden(input_layer, output_gradient, learning_rate=0.001)\n",
    "\n",
    "    print('weights:')\n",
    "    print(output_layer.weights)\n",
    "    print(layer_1.weights)\n",
    "    print('preds:')\n",
    "    print(output_layer.forward_prop(layer_1.forward_prop(input_layer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  69 -420]]\n",
      "[[ 0.84090483 -4.14991045]]\n",
      "[[  88.14803729 -536.82539213]]\n",
      "[[  63.62071304 -387.1800666 ]]\n",
      "[[  70.51121119 -429.22015333]]\n",
      "[[  68.57545316 -417.40976843]]\n",
      "[[  69.11926858 -420.72767766]]\n",
      "[[  68.9664937  -419.79557242]]\n",
      "[[  69.00941298 -420.05743015]]\n",
      "[[  68.9973556  -419.98386606]]\n",
      "[[  69.0007429  -420.00453253]]\n",
      "[[  68.9997913  -419.99872667]]\n"
     ]
    }
   ],
   "source": [
    "target = np.array([[69, -420]])\n",
    "print(target)\n",
    "print(output_layer.forward_prop(layer_1_output))\n",
    "for i in range(10):\n",
    "    _ = output_layer.back_prop_output(layer_1_output, target, learning_rate = 0.1)\n",
    "    print(output_layer.forward_prop(layer_1_output))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
