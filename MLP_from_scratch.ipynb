{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class FullyConnectedLayer:\n",
    "    def __init__(self, input_size, layer_width, \n",
    "                 weights = False, biases = False, \n",
    "                 activation_type = 'linear'):\n",
    "        if isinstance(weights, np.ndarray):\n",
    "            #check if weights are the right size\n",
    "            if (len(weights.shape) == 2) and (weights.shape[0] == input_size) and (weights.shape[1] == layer_width):\n",
    "                self.weights = weights\n",
    "            else:\n",
    "                raise Exception('Weight dimensions look wrong')\n",
    "        else:\n",
    "            self.weights = np.random.rand(input_size, layer_width) #use rand to init\n",
    "        \n",
    "        self.activation_type = activation_type\n",
    "        if self.activation_type == 'linear':\n",
    "            self.activation_function = lambda x : x\n",
    "        elif self.activation_type == 'relu':\n",
    "            self.activation_function = lambda x : np.maximum(x, 0)\n",
    "        elif self.activation_type == 'sigmoid':\n",
    "            self.activation_function = lambda x : 1 / (1 + np.exp(-1 * x))\n",
    "        elif self.activation_type == 'softmax':\n",
    "            self.activation_function = lambda x : np.exp(x) / np.sum(np.exp(x), axis = 1)\n",
    "        else:\n",
    "            raise Exception('Invalid activation function')\n",
    "        \n",
    "        if isinstance(biases, np.ndarray):\n",
    "            if (len(biases.shape) == 2) and (biases.shape[0] == 1) and (biases.shape[1] == layer_width):\n",
    "                self.biases = biases\n",
    "            else:\n",
    "                raise Exception('Bias dimensions look wrong')\n",
    "        else:\n",
    "            self.biases = np.random.rand(1, layer_width)\n",
    "            \n",
    "    def forward_prop(self, input_data):\n",
    "        layer_input = np.add(np.matmul(input_data, self.weights), self.biases)\n",
    "        layer_output = self.activation_function(layer_input)\n",
    "        return layer_output\n",
    "        \n",
    "    def back_prop_output(self, input_data, target, cost_function, learning_rate = 0.001, train = True):\n",
    "        if cost_function == 'square_loss':\n",
    "            cost_gradient = -2 * (target - self.forward_prop(input_data))\n",
    "        elif (cost_function == 'cross_entropy') and (self.activation_type == 'softmax'):\n",
    "            error = self.forward_prop(input_data) - target\n",
    "        elif (cost_function == 'cross_entropy') and (self.activation_type == 'sigmoid'):\n",
    "            error = self.forward_prop(input_data) - target\n",
    "        else:\n",
    "            raise Exception('Invalid cost function')\n",
    "            \n",
    "        if self.activation_type == 'linear':\n",
    "            error = cost_gradient * 1\n",
    "        elif self.activation_type == 'sigmoid':\n",
    "            error = error\n",
    "        elif self.activation_type == 'softmax':\n",
    "            error = error\n",
    "        elif self.activation_type == 'relu':\n",
    "            relu_grad = map(lambda x : 1 if x > 0 else 0, self.forward_prop(input_data).flatten())\n",
    "            error = np.multiply(cost_gradient, np.array([list(relu_grad)]))\n",
    "        else:\n",
    "            raise Exception('Invalid activation function')\n",
    "            \n",
    "        if not train:\n",
    "            return np.matmul(error, self.weights.T)\n",
    "        \n",
    "        weights_update = [] #add updates by neuron, replace this with a single elementise mult\n",
    "        for i in range(target.shape[1]):\n",
    "            weights_update.append(np.multiply(error[0,i], input_data).T)\n",
    "        weights_update = np.concatenate(weights_update, axis = 1)\n",
    "        \n",
    "        self.weights += -1 * learning_rate * weights_update\n",
    "        \n",
    "        self.biases += -1 * learning_rate * error\n",
    "        \n",
    "        return np.matmul(error, self.weights.T)\n",
    "        \n",
    "    def back_prop_hidden(self, input_data, next_layer_gradients, learning_rate = 0.001):\n",
    "        if self.activation_type == 'linear':\n",
    "            error = next_layer_gradients * 1\n",
    "        elif self.activation_type == 'sigmoid':\n",
    "            error = next_layer_gradients * self.forward_prop(input_data) * (1 - self.forward_prop(input_data))\n",
    "        elif self.activation_type == 'relu':\n",
    "            relu_grad = map(lambda x : 1 if x > 0 else -0.1, self.forward_prop(input_data).flatten())\n",
    "            error = next_layer_gradients * np.array([list(relu_grad)])\n",
    "        else:\n",
    "            raise Exception('Invalid activation function')\n",
    "            \n",
    "        weights_update = []\n",
    "        for i in range(error.shape[1]):\n",
    "            weights_update.append(np.multiply(error[0,i], input_data).T)\n",
    "        \n",
    "        weights_update = np.concatenate(weights_update, axis = 1)\n",
    "        \n",
    "        self.weights += -1 * learning_rate * weights_update\n",
    "        \n",
    "        self.biases += -1 * learning_rate * error\n",
    "        \n",
    "        return np.matmul(error, self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = np.array([[1, 1]])\n",
    "\n",
    "layer_1 = FullyConnectedLayer(2, 3, \n",
    "                              activation_type = 'relu'\n",
    "                             )\n",
    "\n",
    "output_layer = FullyConnectedLayer(3, 2,\n",
    "                              activation_type = 'linear'\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target:\n",
      "[[-100  100]]\n",
      "[[-99.99995287 100.00004762]]\n",
      "weights:\n",
      "[[-1.63932624  2.55927905]\n",
      " [-0.52741058  1.49169219]\n",
      " [-5.70839765  5.10540074]]\n",
      "[[1.93412548 0.93371711 5.22570308]\n",
      " [2.13425639 0.81719704 4.92769156]]\n",
      "preds:\n",
      "[[-99.99997995 100.00002026]]\n",
      "weights:\n",
      "[[-1.63932648  2.55927881]\n",
      " [-0.5274107   1.49169208]\n",
      " [-5.70839827  5.10540012]]\n",
      "[[1.93412544 0.93371707 5.2257031 ]\n",
      " [2.13425635 0.81719701 4.92769158]]\n",
      "preds:\n",
      "[[-99.99999147 100.00000862]]\n",
      "weights:\n",
      "[[-1.63932658  2.5592787 ]\n",
      " [-0.52741074  1.49169203]\n",
      " [-5.70839853  5.10539985]]\n",
      "[[1.93412543 0.93371705 5.22570311]\n",
      " [2.13425633 0.81719699 4.92769159]]\n",
      "preds:\n",
      "[[-99.99999637 100.00000367]]\n",
      "weights:\n",
      "[[-1.63932662  2.55927866]\n",
      " [-0.52741076  1.49169201]\n",
      " [-5.70839864  5.10539974]]\n",
      "[[1.93412542 0.93371705 5.22570311]\n",
      " [2.13425633 0.81719698 4.9276916 ]]\n",
      "preds:\n",
      "[[-99.99999846 100.00000156]]\n",
      "weights:\n",
      "[[-1.63932664  2.55927864]\n",
      " [-0.52741077  1.491692  ]\n",
      " [-5.70839869  5.10539969]]\n",
      "[[1.93412542 0.93371704 5.22570311]\n",
      " [2.13425632 0.81719698 4.9276916 ]]\n",
      "preds:\n",
      "[[-99.99999934 100.00000066]]\n",
      "weights:\n",
      "[[-1.63932665  2.55927863]\n",
      " [-0.52741078  1.49169199]\n",
      " [-5.70839871  5.10539967]]\n",
      "[[1.93412541 0.93371704 5.22570311]\n",
      " [2.13425632 0.81719698 4.9276916 ]]\n",
      "preds:\n",
      "[[-99.99999972 100.00000028]]\n",
      "weights:\n",
      "[[-1.63932665  2.55927863]\n",
      " [-0.52741078  1.49169199]\n",
      " [-5.70839872  5.10539966]]\n",
      "[[1.93412541 0.93371704 5.22570311]\n",
      " [2.13425632 0.81719698 4.9276916 ]]\n",
      "preds:\n",
      "[[-99.99999988 100.00000012]]\n",
      "weights:\n",
      "[[-1.63932665  2.55927863]\n",
      " [-0.52741078  1.49169199]\n",
      " [-5.70839872  5.10539966]]\n",
      "[[1.93412541 0.93371704 5.22570311]\n",
      " [2.13425632 0.81719698 4.9276916 ]]\n",
      "preds:\n",
      "[[-99.99999995 100.00000005]]\n",
      "weights:\n",
      "[[-1.63932665  2.55927863]\n",
      " [-0.52741078  1.49169199]\n",
      " [-5.70839873  5.10539965]]\n",
      "[[1.93412541 0.93371704 5.22570311]\n",
      " [2.13425632 0.81719698 4.9276916 ]]\n",
      "preds:\n",
      "[[-99.99999998 100.00000002]]\n",
      "weights:\n",
      "[[-1.63932665  2.55927863]\n",
      " [-0.52741078  1.49169199]\n",
      " [-5.70839873  5.10539965]]\n",
      "[[1.93412541 0.93371704 5.22570311]\n",
      " [2.13425632 0.81719698 4.9276916 ]]\n",
      "preds:\n",
      "[[-99.99999999 100.00000001]]\n"
     ]
    }
   ],
   "source": [
    "target = np.array([[-100, 100]])\n",
    "print('target:')\n",
    "print(target)\n",
    "print(output_layer.forward_prop(layer_1.forward_prop(input_layer)))\n",
    "for i in range(10):\n",
    "    output_gradient = output_layer.back_prop_output(layer_1.forward_prop(input_layer), target, 'square_loss', \n",
    "                                                    learning_rate = 0.001, train = True)\n",
    "    layer_1.back_prop_hidden(input_layer, output_gradient, learning_rate=0.001)\n",
    "\n",
    "    print('weights:')\n",
    "    print(output_layer.weights)\n",
    "    print(layer_1.weights)\n",
    "    print('preds:')\n",
    "    print(output_layer.forward_prop(layer_1.forward_prop(input_layer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  69 -420]]\n",
      "[[ 0.84090483 -4.14991045]]\n",
      "[[  88.14803729 -536.82539213]]\n",
      "[[  63.62071304 -387.1800666 ]]\n",
      "[[  70.51121119 -429.22015333]]\n",
      "[[  68.57545316 -417.40976843]]\n",
      "[[  69.11926858 -420.72767766]]\n",
      "[[  68.9664937  -419.79557242]]\n",
      "[[  69.00941298 -420.05743015]]\n",
      "[[  68.9973556  -419.98386606]]\n",
      "[[  69.0007429  -420.00453253]]\n",
      "[[  68.9997913  -419.99872667]]\n"
     ]
    }
   ],
   "source": [
    "target = np.array([[69, -420]])\n",
    "print(target)\n",
    "print(output_layer.forward_prop(layer_1_output))\n",
    "for i in range(10):\n",
    "    _ = output_layer.back_prop_output(layer_1_output, target, learning_rate = 0.1)\n",
    "    print(output_layer.forward_prop(layer_1_output))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
